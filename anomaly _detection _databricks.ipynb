# ===============================================================
#               DATABRICKS ANOMALY DETECTION PIPELINE
# ===============================================================
# This notebook demonstrates an end-to-end anomaly detection 
# pipeline including:
#   1) Data ingestion from lake-style storage
#   2) Technical Data Quality (TDQ) checks
#   3) Business Data Quality (BDQ) checks
#   4) Time-series anomaly detection using ML
#   5) Final risk scoring and output
# ===============================================================
# NOTE: This is a Databricks-style notebook written in PySpark.
# ===============================================================


# --------------------------
# 1. IMPORT LIBRARIES
# --------------------------
from pyspark.sql.functions import *
from pyspark.sql.types import *
import pandas as pd
from sklearn.ensemble import IsolationForest


# --------------------------
# 2. INGEST DATA (SIMULATED)
# --------------------------
# In real Databricks, this would read from Data Lake / Snowflake
sales_df = spark.read.csv("../data/sales.csv", header=True, inferSchema=True)
inventory_df = spark.read.csv("../data/inventory.csv", header=True, inferSchema=True)


# --------------------------
# 3. TECHNICAL DATA QUALITY (TDQ)
# --------------------------
tdq_report = {
    "sales_nulls": {c: sales_df.filter(col(c).isNull()).count() for c in sales_df.columns},
    "sales_duplicates": sales_df.count() - sales_df.dropDuplicates().count(),
    "inventory_nulls": {c: inventory_df.filter(col(c).isNull()).count() for c in inventory_df.columns},
    "inventory_duplicates": inventory_df.count() - inventory_df.dropDuplicates().count()
}

tdq_df = spark.createDataFrame(
    [(str(tdq_report),)],
    ["tdq_summary"]
)


# --------------------------
# 4. BUSINESS DATA QUALITY (BDQ)
# --------------------------

bdq_sales = sales_df.withColumn(
    "bdq_flag",
    when(col("amount") <= 0, "INVALID_AMOUNT")
    .when(col("date") > current_date(), "FUTURE_DATE")
    .otherwise("PASS")
)

bdq_inventory = inventory_df.withColumn(
    "bdq_flag",
    when(col("stock") < 0, "NEGATIVE_STOCK")
    .otherwise("PASS")
)


# --------------------------
# 5. TIME SERIES ANOMALY DETECTION
# --------------------------

# Select numeric features
pdf = sales_df.select("amount", "quantity").dropna().toPandas()

# Train anomaly detection model
model = IsolationForest(contamination=0.03, random_state=42)
model.fit(pdf)

# Predict anomalies
pdf["anomaly_score"] = model.decision_function(pdf)
pdf["is_anomaly"] = model.predict(pdf)  # -1 = anomaly

anomaly_df = spark.createDataFrame(pdf)


# --------------------------
# 6. PREDICTIVE RULES
# --------------------------

rules_df = anomaly_df.withColumn(
    "risk_flag",
    when(col("is_anomaly") == -1, "ML_ANOMALY")
    .when(col("amount") > 50000, "HIGH_VALUE")
    .otherwise("NORMAL")
)


# --------------------------
# 7. FINAL OUTPUT
# --------------------------

final_df = rules_df

final_df.write.mode("overwrite").format("delta").save("../output/final_results")

display(final_df)
